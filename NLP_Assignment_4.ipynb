{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Create a transformer from scratch using the Pytorch library**"
      ],
      "metadata": {
        "id": "YaKtKhqJwI_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "pcZfONP1j_ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Head Attention module\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert embed_size % num_heads == 0, \"Embedding size must be divisible by the number of heads\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_size // num_heads\n",
        "\n",
        "        self.query = nn.Linear(embed_size, embed_size)\n",
        "        self.key = nn.Linear(embed_size, embed_size)\n",
        "        self.value = nn.Linear(embed_size, embed_size)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, queries, keys, values, mask=None):\n",
        "        N = queries.shape[0]\n",
        "        Q = self.query(queries)\n",
        "        K = self.key(keys)\n",
        "        V = self.value(values)\n",
        "\n",
        "        # Split into heads\n",
        "        Q = Q.view(N, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(N, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(N, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        energy = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float('-1e20'))\n",
        "\n",
        "        attention = torch.softmax(energy, dim=-1)\n",
        "        out = torch.matmul(attention, V)\n",
        "\n",
        "        # Concatenate heads\n",
        "        out = out.transpose(1, 2).contiguous().view(N, -1, self.num_heads * self.head_dim)\n",
        "\n",
        "        # Final linear layer\n",
        "        out = self.fc_out(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "uWqcz6uzmFUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Position-wise Feed-Forward Network\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, embed_size, ff_dim):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(embed_size, ff_dim)\n",
        "        self.fc2 = nn.Linear(ff_dim, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(F.relu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "CJQdLxKMmJ-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_size, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.encoding = torch.zeros(max_len, embed_size)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * -(math.log(10000.0) / embed_size))\n",
        "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.encoding = self.encoding.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        return x + self.encoding[:, :seq_len, :].to(x.device)"
      ],
      "metadata": {
        "id": "46Mj0pZJmLoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Encoder Layer\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads, ff_dim, dropout):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(embed_size, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.feed_forward = PositionWiseFeedForward(embed_size, ff_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Self-attention\n",
        "        attn_out = self.attention(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "\n",
        "        # Feed-forward\n",
        "        ff_out = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_out))\n",
        "        return x"
      ],
      "metadata": {
        "id": "UrNi3E7EmM4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full Transformer Encoder\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads, ff_dim, num_layers, vocab_size, max_len, dropout):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.positional_encoding = PositionalEncoding(embed_size, max_len)\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(embed_size, num_heads, ff_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.embedding(x) * math.sqrt(self.embed_size)\n",
        "        x = self.positional_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "CNf4k2Z4mPGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Model\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_size, num_heads, num_layers, ff_dim, max_len, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, embed_size)\n",
        "        self.positional_encoding = PositionalEncoding(embed_size, max_len)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([TransformerEncoderLayer(embed_size, num_heads, ff_dim, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([TransformerEncoderLayer(embed_size, num_heads, ff_dim, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc_out = nn.Linear(embed_size, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, None)\n",
        "\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, None)\n",
        "\n",
        "        output = self.fc_out(dec_output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "oE_9Q05CmOI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "src_vocab_size = 5000\n",
        "tgt_vocab_size = 5000\n",
        "embed_size = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "ff_dim = 2048\n",
        "max_seq_length = 100\n",
        "dropout = 0.1\n",
        "epochs = 15\n",
        "\n",
        "# Initialize model\n",
        "transformer = Transformer(src_vocab_size, tgt_vocab_size, embed_size, num_heads, num_layers, ff_dim, max_seq_length, dropout)\n",
        "\n",
        "# Generate random sample data\n",
        "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
        "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Training loop\n",
        "transformer.train()\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    output = transformer(src_data, tgt_data[:, :-1])\n",
        "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluation\n",
        "transformer.eval()\n",
        "val_src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))\n",
        "val_tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))\n",
        "\n",
        "with torch.no_grad():\n",
        "    val_output = transformer(val_src_data, val_tgt_data[:, :-1])\n",
        "    val_loss = criterion(val_output.contiguous().view(-1, tgt_vocab_size), val_tgt_data[:, 1:].contiguous().view(-1))\n",
        "    print(f\"Validation Loss: {val_loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJ-uj83mmTg9",
        "outputId": "741d789f-5b40-421a-aef7-10719353c03c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 8.67827033996582\n",
            "Epoch: 2, Loss: 8.565383911132812\n",
            "Epoch: 3, Loss: 8.477017402648926\n",
            "Epoch: 4, Loss: 8.405959129333496\n",
            "Epoch: 5, Loss: 8.342824935913086\n",
            "Epoch: 6, Loss: 8.276484489440918\n",
            "Epoch: 7, Loss: 8.205962181091309\n",
            "Epoch: 8, Loss: 8.125167846679688\n",
            "Epoch: 9, Loss: 8.0442533493042\n",
            "Epoch: 10, Loss: 7.964663505554199\n",
            "Epoch: 11, Loss: 7.888030529022217\n",
            "Epoch: 12, Loss: 7.807408332824707\n",
            "Epoch: 13, Loss: 7.731207370758057\n",
            "Epoch: 14, Loss: 7.652472496032715\n",
            "Epoch: 15, Loss: 7.573073387145996\n",
            "Validation Loss: 8.688231468200684\n"
          ]
        }
      ]
    }
  ]
}