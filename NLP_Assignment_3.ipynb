{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Perform text cleaning, perform lemmatization (any method), remove stop words (any method), label encoding. Create representations using TF-IDF. Save outputs**"
      ],
      "metadata": {
        "id": "Nv5Xu2VaWAFY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6FzXVEgmN1e",
        "outputId": "86eac966-fb31-4112-c37b-0627590659d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import nltk\n",
        "import string\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pickle\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text data\n",
        "corpus = [\n",
        "    \"Natural Language Processing (NLP) is fun and exciting.\",\n",
        "    \"Machine learning is a subfield of artificial intelligence.\",\n",
        "    \"Deep learning is a part of machine learning, involving neural networks.\",\n",
        "    \"NLP techniques include tokenization, stemming, lemmatization, and stopword removal.\"\n",
        "]\n",
        "\n",
        "# Sample labels for text classification (e.g., categories)\n",
        "labels = ['Technology', 'Technology', 'AI', 'AI']\n"
      ],
      "metadata": {
        "id": "kRVjQ947mely"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean the text\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation using string.punctuation\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])\n",
        "    return text\n",
        "\n",
        "# Clean the corpus\n",
        "cleaned_corpus = [clean_text(text) for text in corpus]\n",
        "print(\"\\nCleaned Text Corpus:\", cleaned_corpus)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ooaS8Nymf40",
        "outputId": "6ddb38a7-df3d-4300-b49f-9fe2cb610f33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cleaned Text Corpus: ['natural language processing nlp is fun and exciting', 'machine learning is a subfield of artificial intelligence', 'deep learning is a part of machine learning involving neural networks', 'nlp techniques include tokenization stemming lemmatization and stopword removal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lemmatizer\n",
        "lemmatizer = nltk.WordNetLemmatizer()\n",
        "\n",
        "# Function to lemmatize the text\n",
        "def lemmatize_text(text):\n",
        "    words = word_tokenize(text)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "# Lemmatize the cleaned corpus\n",
        "lemmatized_corpus = [lemmatize_text(text) for text in cleaned_corpus]\n",
        "print(\"\\nLemmatized Text Corpus:\", lemmatized_corpus)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJpj-c0fmg54",
        "outputId": "c922ff09-1612-497c-8bd2-18fc339879bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatized Text Corpus: ['natural language processing nlp is fun and exciting', 'machine learning is a subfield of artificial intelligence', 'deep learning is a part of machine learning involving neural network', 'nlp technique include tokenization stemming lemmatization and stopword removal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get stop words from NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to remove stop words\n",
        "def remove_stopwords(text):\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Remove stop words from the lemmatized corpus\n",
        "cleaned_no_stopwords = [remove_stopwords(text) for text in lemmatized_corpus]\n",
        "print(\"\\nText without Stopwords:\", cleaned_no_stopwords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB_Km_Qjmh74",
        "outputId": "6fb02c3e-18cf-49a8-94b2-9d7ea3a9c533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Text without Stopwords: ['natural language processing nlp fun exciting', 'machine learning subfield artificial intelligence', 'deep learning part machine learning involving neural network', 'nlp technique include tokenization stemming lemmatization stopword removal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encode the labels\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "print(\"\\nEncoded Labels:\", encoded_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSHGzt8jmi-N",
        "outputId": "e024f538-ddd0-425a-ef64-d7dde9995e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoded Labels: [1 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer and transform the cleaned text into a TF-IDF representation\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_no_stopwords)\n",
        "\n",
        "# Convert to an array for easier visualization\n",
        "tfidf_matrix = X_tfidf.toarray()\n",
        "print(\"\\nTF-IDF Matrix:\\n\", tfidf_matrix)\n",
        "\n",
        "# Display the feature names (words) from the TF-IDF matrix\n",
        "print(\"\\nTF-IDF Feature Names (Words):\", tfidf_vectorizer.get_feature_names_out())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqKWJSZKmj_5",
        "outputId": "4c1cce8a-fc18-45a9-87c8-c7845c0a7a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF Matrix:\n",
            " [[0.         0.         0.42176478 0.42176478 0.         0.\n",
            "  0.         0.42176478 0.         0.         0.         0.42176478\n",
            "  0.         0.         0.3325242  0.         0.42176478 0.\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.48546061 0.         0.         0.         0.         0.48546061\n",
            "  0.         0.         0.38274272 0.         0.38274272 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.48546061 0.         0.        ]\n",
            " [0.         0.35119159 0.         0.         0.         0.\n",
            "  0.35119159 0.         0.55376697 0.         0.27688349 0.\n",
            "  0.35119159 0.35119159 0.         0.35119159 0.         0.\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.36222393 0.\n",
            "  0.         0.         0.         0.36222393 0.         0.\n",
            "  0.         0.         0.2855815  0.         0.         0.36222393\n",
            "  0.36222393 0.36222393 0.         0.36222393 0.36222393]]\n",
            "\n",
            "TF-IDF Feature Names (Words): ['artificial' 'deep' 'exciting' 'fun' 'include' 'intelligence' 'involving'\n",
            " 'language' 'learning' 'lemmatization' 'machine' 'natural' 'network'\n",
            " 'neural' 'nlp' 'part' 'processing' 'removal' 'stemming' 'stopword'\n",
            " 'subfield' 'technique' 'tokenization']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save cleaned text to a file\n",
        "with open('cleaned_corpus.txt', 'w') as file:\n",
        "    for text in cleaned_corpus:\n",
        "        file.write(text + '\\n')\n",
        "\n",
        "# Save lemmatized text to a file\n",
        "with open('lemmatized_corpus.txt', 'w') as file:\n",
        "    for text in lemmatized_corpus:\n",
        "        file.write(text + '\\n')\n",
        "\n",
        "# Save text without stopwords to a file\n",
        "with open('no_stopwords_corpus.txt', 'w') as file:\n",
        "    for text in cleaned_no_stopwords:\n",
        "        file.write(text + '\\n')\n",
        "\n",
        "# Save TF-IDF matrix to a CSV file using pandas\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix, columns=tfidf_vectorizer.get_feature_names_out())\n",
        "tfidf_df.to_csv('tfidf_matrix.csv', index=False)\n",
        "\n",
        "# Save label encoder using pickle\n",
        "with open('label_encoder.pkl', 'wb') as file:\n",
        "    pickle.dump(label_encoder, file)\n",
        "\n",
        "print(\"\\nOutputs have been saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEUgJ2PlmlSv",
        "outputId": "da77addb-9fb4-4241-feb2-2d767a1ed549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Outputs have been saved!\n"
          ]
        }
      ]
    }
  ]
}