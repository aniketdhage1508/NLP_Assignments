{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Perform bag-of-words approach (count occurrence, normalized count occurrence), TF-IDF on data. Create embeddings using Word2Vec**"
      ],
      "metadata": {
        "id": "Zw_ktHECV62O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO_30WXadUst",
        "outputId": "dc18cc64-e1a9-405c-f88f-83e842bdb7d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "\n",
        "# Ensure you have the necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text data (list of sentences)\n",
        "corpus = [\n",
        "    \"Natural Language Processing is fun.\",\n",
        "    \"Machine learning is a subfield of artificial intelligence.\",\n",
        "    \"Deep learning is a part of machine learning.\",\n",
        "    \"NLP techniques include tokenization, stemming, and lemmatization.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "67526kDwdVTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize CountVectorizer to count word occurrences\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit the vectorizer on the corpus and transform the text data\n",
        "X_count = count_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Convert to an array and print the result\n",
        "count_matrix = X_count.toarray()\n",
        "print(\"Count Occurrence (Bag-of-Words):\\n\", count_matrix)\n",
        "\n",
        "# Display the feature names (words)\n",
        "print(\"\\nFeature Names (Words):\", count_vectorizer.get_feature_names_out())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0zdhgCzdWf1",
        "outputId": "09bc1286-c981-4b1a-b462-f8f1d1b779e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count Occurrence (Bag-of-Words):\n",
            " [[0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0]\n",
            " [0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0]\n",
            " [0 0 1 0 0 0 1 0 2 0 1 0 0 1 1 0 0 0 0 0]\n",
            " [1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1]]\n",
            "\n",
            "Feature Names (Words): ['and' 'artificial' 'deep' 'fun' 'include' 'intelligence' 'is' 'language'\n",
            " 'learning' 'lemmatization' 'machine' 'natural' 'nlp' 'of' 'part'\n",
            " 'processing' 'stemming' 'subfield' 'techniques' 'tokenization']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize word occurrences by converting the count matrix into term frequency\n",
        "X_normalized = X_count.toarray() / X_count.sum(axis=1).reshape(-1, 1)\n",
        "\n",
        "print(\"\\nNormalized Count Occurrence (Term Frequency):\\n\", X_normalized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drwlFhv-dX50",
        "outputId": "65e4c26e-4589-4556-9fde-84ccc8431759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Normalized Count Occurrence (Term Frequency):\n",
            " [[0.         0.         0.         0.2        0.         0.\n",
            "  0.2        0.2        0.         0.         0.         0.2\n",
            "  0.         0.         0.         0.2        0.         0.\n",
            "  0.         0.        ]\n",
            " [0.         0.14285714 0.         0.         0.         0.14285714\n",
            "  0.14285714 0.         0.14285714 0.         0.14285714 0.\n",
            "  0.         0.14285714 0.         0.         0.         0.14285714\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.14285714 0.         0.         0.\n",
            "  0.14285714 0.         0.28571429 0.         0.14285714 0.\n",
            "  0.         0.14285714 0.14285714 0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.14285714 0.         0.         0.         0.14285714 0.\n",
            "  0.         0.         0.         0.14285714 0.         0.\n",
            "  0.14285714 0.         0.         0.         0.14285714 0.\n",
            "  0.14285714 0.14285714]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize TfidfVectorizer to calculate TF-IDF scores\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer on the corpus and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Convert to an array and print the result\n",
        "tfidf_matrix = X_tfidf.toarray()\n",
        "print(\"\\nTF-IDF Matrix:\\n\", tfidf_matrix)\n",
        "\n",
        "# Display the feature names (words)\n",
        "print(\"\\nTF-IDF Feature Names (Words):\", tfidf_vectorizer.get_feature_names_out())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTp5M6FPdY_s",
        "outputId": "a8da4073-ada2-436f-c351-2f80c848c59f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF Matrix:\n",
            " [[0.         0.         0.         0.47633035 0.         0.\n",
            "  0.30403549 0.47633035 0.         0.         0.         0.47633035\n",
            "  0.         0.         0.         0.47633035 0.         0.\n",
            "  0.         0.        ]\n",
            " [0.         0.43551643 0.         0.         0.         0.43551643\n",
            "  0.27798449 0.         0.34336615 0.         0.34336615 0.\n",
            "  0.         0.34336615 0.         0.         0.         0.43551643\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.40366689 0.         0.         0.\n",
            "  0.25765535 0.         0.63651122 0.         0.31825561 0.\n",
            "  0.         0.31825561 0.40366689 0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.37796447 0.         0.         0.         0.37796447 0.\n",
            "  0.         0.         0.         0.37796447 0.         0.\n",
            "  0.37796447 0.         0.         0.         0.37796447 0.\n",
            "  0.37796447 0.37796447]]\n",
            "\n",
            "TF-IDF Feature Names (Words): ['and' 'artificial' 'deep' 'fun' 'include' 'intelligence' 'is' 'language'\n",
            " 'learning' 'lemmatization' 'machine' 'natural' 'nlp' 'of' 'part'\n",
            " 'processing' 'stemming' 'subfield' 'techniques' 'tokenization']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the corpus (split text into words)\n",
        "tokenized_corpus = [word_tokenize(doc.lower()) for doc in corpus]\n",
        "\n",
        "# Initialize the Word2Vec model\n",
        "word2vec_model = Word2Vec(tokenized_corpus, vector_size=50, window=3, min_count=1, sg=0)\n",
        "\n",
        "# Train the Word2Vec model\n",
        "word2vec_model.train(tokenized_corpus, total_examples=len(corpus), epochs=10)\n",
        "\n",
        "# Get the vector representation (embedding) for a specific word\n",
        "word_vector = word2vec_model.wv['language']  # Example word: 'language'\n",
        "print(\"\\nWord2Vec Embedding for 'language':\\n\", word_vector)\n",
        "\n",
        "# Get the vector representation for a list of words\n",
        "words = ['language', 'machine', 'learning']\n",
        "word_vectors = [word2vec_model.wv[word] for word in words]\n",
        "print(\"\\nWord2Vec Embeddings for multiple words:\", list(zip(words, word_vectors)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHdwr19GdaHU",
        "outputId": "b1599fd9-19c1-42a8-84ab-640a97b3070f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word2Vec Embedding for 'language':\n",
            " [-0.01428453  0.00248143 -0.01435139 -0.00447787  0.00744023  0.01165937\n",
            "  0.00240805  0.00420501 -0.0082356   0.01444217 -0.01261733  0.00928294\n",
            " -0.016431    0.00407175 -0.00994886 -0.00848354 -0.00621368  0.01131507\n",
            "  0.01157967 -0.00995801  0.00154655 -0.01698448  0.01563551  0.01850453\n",
            " -0.00548207  0.00159986  0.00148957  0.01096059 -0.01722168  0.0011757\n",
            "  0.01374442  0.00445443  0.00224918 -0.01864685  0.016965   -0.01252744\n",
            " -0.00597483  0.00698619 -0.00154332  0.00282392  0.00356977 -0.01366348\n",
            " -0.01945646  0.01807972  0.01240432 -0.01382146  0.00681484  0.0004075\n",
            "  0.00950946 -0.01423734]\n",
            "\n",
            "Word2Vec Embeddings for multiple words: [('language', array([-0.01428453,  0.00248143, -0.01435139, -0.00447787,  0.00744023,\n",
            "        0.01165937,  0.00240805,  0.00420501, -0.0082356 ,  0.01444217,\n",
            "       -0.01261733,  0.00928294, -0.016431  ,  0.00407175, -0.00994886,\n",
            "       -0.00848354, -0.00621368,  0.01131507,  0.01157967, -0.00995801,\n",
            "        0.00154655, -0.01698448,  0.01563551,  0.01850453, -0.00548207,\n",
            "        0.00159986,  0.00148957,  0.01096059, -0.01722168,  0.0011757 ,\n",
            "        0.01374442,  0.00445443,  0.00224918, -0.01864685,  0.016965  ,\n",
            "       -0.01252744, -0.00597483,  0.00698619, -0.00154332,  0.00282392,\n",
            "        0.00356977, -0.01366348, -0.01945646,  0.01807972,  0.01240432,\n",
            "       -0.01382146,  0.00681484,  0.0004075 ,  0.00950946, -0.01423734],\n",
            "      dtype=float32)), ('machine', array([ 2.87488988e-03, -5.29733393e-03, -1.41525380e-02, -1.56056909e-02,\n",
            "       -1.82493422e-02, -1.18754115e-02, -3.68545903e-03, -8.65150057e-03,\n",
            "       -1.29292114e-02, -7.44032301e-03,  8.57848395e-03, -7.48003786e-03,\n",
            "        1.67671051e-02,  3.06209153e-03, -1.44803962e-02,  1.88722815e-02,\n",
            "        1.52609237e-02,  1.09911682e-02, -1.37088625e-02,  1.16325906e-02,\n",
            "        8.02334305e-03,  1.03792856e-02,  8.51551071e-03,  3.87955736e-03,\n",
            "       -6.33719657e-03,  1.67048108e-02,  1.92205645e-02,  7.59324897e-03,\n",
            "       -5.68167632e-03,  1.77275069e-05,  2.43748142e-03, -1.69165097e-02,\n",
            "       -1.64432339e-02, -4.59842558e-04,  2.46981578e-03, -1.14919906e-02,\n",
            "       -9.44005605e-03, -1.46914870e-02,  1.66506600e-02,  2.39148838e-04,\n",
            "       -9.01849568e-03,  1.14034265e-02,  1.83556322e-02, -8.20210762e-03,\n",
            "        1.59421135e-02,  1.07526975e-02,  1.17612751e-02,  1.01255823e-03,\n",
            "        1.64255444e-02, -1.40361786e-02], dtype=float32)), ('learning', array([-0.01723901,  0.00733142,  0.01038799,  0.01149917,  0.0149321 ,\n",
            "       -0.01234471,  0.00223528,  0.01209522, -0.0056937 , -0.01236819,\n",
            "       -0.00081955, -0.01674673, -0.01117754,  0.01419727,  0.00671288,\n",
            "        0.0144604 ,  0.0136077 ,  0.01507432, -0.00761104, -0.00114512,\n",
            "        0.00469965, -0.0090247 ,  0.01680176, -0.01972448,  0.01351802,\n",
            "        0.00583254, -0.00988309,  0.00880767, -0.00349927,  0.01343307,\n",
            "        0.01992945, -0.00873425, -0.00118872, -0.01139917,  0.00770048,\n",
            "        0.00557237,  0.01379293,  0.01220319,  0.01906823,  0.01855107,\n",
            "        0.01580602, -0.01398042, -0.01831812, -0.00071479, -0.00616726,\n",
            "        0.01579731,  0.01188168, -0.00310351,  0.00303325,  0.00357643],\n",
            "      dtype=float32))]\n"
          ]
        }
      ]
    }
  ]
}